---
title: "Fundamentals of Computing and Data Display"
author: "Rameesha Mehboob and Abigail Kamp"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    df_print: kable
    toc: yes
subtitle: SURV727 Mehboob and Kamp Final Presentation
---

## Introduction

In 2018, Initiative 1631 was on the ballot in Washington state. It asked voters:

"This measure would charge pollution fees on sources of greenhouse gas pollutants and use the revenue to reduce pollution, promote clean energy, and address climate impacts, under oversight of a public board.

Should this measure be enacted into law?""

The Initiative would have enacted a fee a $15 per ton of carbon emitted in 2020 and rise $2 per year until 2035. The revenue would have been invested in "clean air and energy", "clean water and health forests", and "healthy communities". Initiative 1631 was supported by a variety of climate groups, and recieved endorsements from many activist groups. There was a broad coalition supporting the measure in a state with left-leaning Washington politics, which gave supporters hope that it would pass. However, like many other tax increases, carbon taxes proved unpopular with voters. The referendum did not pass (56% to 44%). (Source:  https://www.vox.com/energy-and-environment/2018/9/28/17899804/washington-1631-results-carbon-fee-green-new-deal)

While this referendum was a defeat for climate activists, it is an interesting case study to compare the online support for a ballot initiative with actual voting results. For our analysis, we collect Twitter data about Initative 1631 and compare it to the actual voting results in Washington state. 

```{r, include = FALSE}
#First we load in our packages
library(rtweet)
library(dplyr)
library(tidyr)
library(readr)
library(rjson)
library(ggmap)
library(maps)
library(ggvis)
library(ggplot2)
library(formattable)
```


## Data

First, we begin by collecting Twitter data. We each applied and obtained Twitter Enterprise accounts, which allowed us to access the entire Twitter archive. (For future researchers, each account is allowed free access to 5,000 Tweets. Once the limit is reached, TWitter will charge for further aceess.)

```{r, include = FALSE, eval = FALSE}
# Abby's Twitter API token. API information removed for security purposes
create_token(
  app = "SURV727",
  consumer_key = "",
  consumer_secret = "",
  access_token = "",
  access_secret = " ")

#get the token
get_token()
```

We pulled the Tweets by the four weeks before the election. We pulled more tweets closer to Election Day (November 06, 2018) because interest in an election generally increases as the date gets closer. 

``` {r, include = FALSE, eval = FALSE}
#Actually pulling the Tweets

##PULLING YES TWEETS##
yes_week1 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810090000",toDate = "201810152359")

yes_week2 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810160000",toDate = "201810222359")

yes_week3 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810230000",toDate = "201810292359")

yes_week4 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four yes objects and saving to one file
yes_4_weeks <- do.call("rbind", list(yes_week1, yes_week2, yes_week3, yes_week4))

##Pulling NEUTRAL Tweet"
neutral_week1 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 100, env_name = "SURV727", 
                                    fromDate ="201810090000",  toDate = "201810152359")

neutral_week2 <- search_fullarchive( "#initiative1631 OR (initiative 1631)", 
                                     n = 100, env_name = "SURV727",
                                     fromDate="201810160000", toDate = "201810222359")

neutral_week3 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810230000",toDate = "201810292359")

neutral_week4 <- search_fullarchive("#initiative1631 OR (initiative 1631)",
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810300000",toDate = "201811062359")

#merging the neutral weeks
netural_4_weeks <- do.call("rbind", list(neutral_week1, neutral_week2, neutral_week3, neutral_week4))

#Pulling the Negative Tweets#
no_week1 <- search_fullarchive("#NoOn1631 OR #Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810090000",toDate = "201810152359")

no_week2 <- search_fullarchive("NoOn1631 OR Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810160000",toDate = "201810222359")

no_week3 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810230000",toDate = "201810292359")

no_week4 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four no objects and saving to one file
no_4_weeks <- do.call("rbind", list(no_week1, no_week2, no_week3, no_week4))

#merging all tweets
all_tweets <- do.call("rbind", list(yes_4_weeks, netural_4_weeks, no_4_weeks))

#we then saved it as a CSV
save_as_csv(all_tweets,"tweets.csv",prepend_ids = TRUE, na = "",fileEncoding = "UTF-8")

```

We do not want to keep running the Twitter search_fullarchive function because it will eventually charge us money to pull more data, so we saved the data in CSV file. Then we removed duplicate tweets. There were about 400 duplicate tweets in the file.

```{r, include = FALSE}
# to avoid running the code again, we uploaded the Tweets file as CSV
#setwd()

tweets <- read_csv("tweets.csv")

#removing duplicates
tweets <- tweets[!duplicated(tweets$status_id), ]
```

We also obtained the user data that was associated with the Twitter accounts in the file above. The "users_data" function in rtweet pulls information about the individual users which we stored in a new file.

```{r, include = FALSE}
#this won't work if you don't have the original R object, so commenting it out and adding our results as a CSV
#users <- users_data(tweets)
#set your working directory to where you have saved your data
#setwd("~/Data-cleaning/final-paper/data")

users <- read_csv("twitter-users.csv")
```

All location data in TWitter is input by the user, so it lacks uniformity and is difficult to work with. However, we start by trying to isolate the city from the user location data. Of note, this information is entered by the user and may be not accurate, but there is no way to verify location. 

To match the Twitter-user entered city, we need to match that with the relevant county information. We obtained a list of Washington cities and their respective counties from the Washington Court System Website (https://www.courts.wa.gov/court_dir/?fa=court_dir.countycityref). The Washington Court system lists each city and their county in alphahabetically order, and we created a CSV from this information. 

```{r, include = FALSE}
#begin cleaning the user data by dropping the NAs and separating into city and state where possible
users <-
  users %>%
   dplyr::select(user_id, location, screen_name) %>%
   drop_na(.)

users <- separate(users, location, into = c("city","state"), sep= (","))

#need to get rid of the white space and creating a new file to do this
users2 <-
users %>%
  dplyr::select(user_id, screen_name, city) %>%
  mutate(city = tolower(trimws(.$city)))
```
Here we merge the cities file with the user file. 

```{r, include= FALSE}
#uploading list of cities in washington to match with user information
cities <- read_csv("washington-cities.csv")

#renaming to match with other
colnames(cities)[1] <- "city"
colnames(cities)[2] <- "county"

#cleaning city data
cities <-
cities %>%
  mutate(city = tolower(trimws(.$city, "l")))

cities$city <- (gsub("\\s", "", cities$city))

str(users$city)
str(cities$city)

#comparing the locations dataframe
full_location <- merge(users2, cities, by = "city")

#reordering city variables
full_location <-
full_location %>%
  dplyr::select(user_id, screen_name, city, county)

#annoyingly full_location has an extra x in the user ID so need to remove
full_location$user_id <- gsub("x", "", full_location$user_id)
tweets$user_id <- gsub("x", "", tweets$user_id)

#merge user location with actual tweets
tweets_location <-
  full_location %>% semi_join(tweets, by = "user_id")

```

To obtain the voting results, we went to the Washington Secretary of State webpage and downloaded the "All Counties" file to obtain the results of each election broken down by county. (source: https://results.vote.wa.gov/results/20181106/Export.html) 

```{r, include = FALSE}
# download the file to your working directly and upload it as a CSV
votes <- read_csv("washington-2018-results.csv")
```


## Results

# Tweet Results

We created an interactive graph that shows the reply count and favorite count of the primary hashtags used by twitter users. Top 6 hashtags were extracted from 1686 tweets and their primary hashtags were the summed up for total favorite count and reply count. 

```{r, warning = FALSE}

popular_tweets <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  group_by(hashtag1) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE)

popular_tweets%>%
  filter(hashtag1 >50)%>%
  ggvis(x= ~favorite_count,y= ~ reply_count) %>%
  filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "electionday",
                                    "climate",
                                    "electionday2018",
                                    "noon1631",
                                    "initiative1631"),
                                    label = "Choose Hashtag:",
                                    selected = "yeson1631"))) %>% 

layer_points(fill = ~factor(hashtag1), size := input_numeric(80 , label = "Point size"))%>%
  add_tooltip(function(popular_tweets){paste0(
    ### <b>
    "Number of Favorite Count received: ", popular_tweets$favorite_count, "<br>",
     ### </b>
    "Number of Reply Count Received: ", popular_tweets$reply_count)}) %>%
  add_axis("x", title_offset = 40, title = "Number of Favorite Count Received", properties = axis_props(title = list(fontSize = 12))) %>%
  add_axis("y", title_offset = 40, title = "Number of Reply Count Received", properties = axis_props(title = list(fontSize = 12)))%>%
  add_axis("x", orient = "top", ticks = 0, title = "Twitter Support",
           properties = axis_props(
             axis = list(stroke = "white"),
             title = list(fontSize = 12),
             labels = list(fontSize = 0)))
  

```

We also looked at the screen_names of twitter users who are most actively participating in tweeting about initiative 1631. The graph shows that there is  greater positive support for Initiative 1631. However most of the support is from several top users; which seems to be from activist groups rather than individuals. 

```{r, warning = FALSE}
user_support <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count,text,screen_name) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  filter(hashtag1=="yeson1631"|hashtag1=="noon1631")%>%
  group_by(screen_name) %>%
  count(hashtag1)



user_support%>%
  filter(n>2)%>%
ggvis(x = ~n, y = ~ screen_name,
       fill = ~continent,
        size := 100,
        opacity := 1) %>%
    filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "noon1631"),
                                    label = "Choose Hashtag:",
                                    selected = "yeson1631"))) %>% 
layer_points(fill = ~factor(hashtag1)) %>%
  add_axis('y', title=' ', properties=axis_props(labels=list(fontSize=12), title=list(fontSize=16,dy=-25)))%>%
  add_tooltip(function(user_support){paste0(
    ### <b>
    "Number of times hashtag was tweeted by user: ", user_support$n)})%>%
  add_axis("x", title_offset = 40, title = "Number of Tweets by Twitter User", properties = axis_props(title = list(fontSize = 12))) %>%
  add_axis("x", orient = "top", ticks = 0, title = "Evaluating Twitter Support with Screen Name ",
           properties = axis_props(
             axis = list(stroke = "white"),
             title = list(fontSize = 12),
             labels = list(fontSize = 0)))

```

We then plot the frequency of different hashtags. I start with the first hashtag that people use in the Tweet. Yeson1631 remains the most frequent hashtag.

```{r, warning = FALSE}

hashes <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag1) %>%
  count(hashtag1) %>%
  arrange(desc(n))

first_tag <- 
hashes %>%
  filter(n >=5) %>%
  ggvis(x = ~n,
        y = ~hashtag1,
        fill= ~n) %>%
  layer_points() %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag1, "<br>", "freq: ", hashes$n) }) %>%
  add_axis("x", title = 'Number of Uses', properties = axis_props(labels = list(angle = 60, 
             align = "left", baseline = "middle"))) %>%
  add_axis("x", orient = "top", ticks = 0, title = "Most Popular Hashtags",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0)))%>%
  add_axis("y", title = '', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))



first_tag
```

Now looking at their second choice hastag to see if the second hashtag mentioned in a post are different from the first. Based on our results, it looks like most of the hashtags remain positive.

```{r, warning = FALSE}

hashes2 <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag2) %>%
  drop_na(hashtag2) %>%
  count(hashtag2) %>%
  arrange(desc(n))

hashes

second_tag <- 
hashes2 %>%
  filter(n >=5) %>%
  ggvis(x = ~n,
        y = ~hashtag2,
        fill= ~n) %>%
  layer_points() %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag2, "<br>", "freq: ", hashes$n) }) %>%
   add_axis("x", orient = "top", ticks = 0, title = "Second Most Popular Hashtags",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0))) %>%
  add_axis("x", title = 'Number of Uses', properties = axis_props
           (labels =list(angle = 60, align = "left", baseline = "middle"))) %>%
   add_axis("y", title = '', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))

second_tag

```

Next we select  the mos popular tweets and we graph the tweets as points to see which are the most favorited and replied to. It will be useful to know if some are retweeted more but aren't favorited, or vice versa.

```{r}
favs <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text, user_id) %>%
  tidyr::drop_na(favorite_count) %>%
  arrange(desc(favorite_count))

favs %>%
  filter(favorite_count >=10) %>%
  ggvis(x = ~favorite_count, y= ~reply_count, key := ~text) %>%
  layer_points() %>%
    add_tooltip(function(favs){paste0(
    "replies: ", favs$reply_count, "<br>",
    "favorites: ", favs$favorite_count, "<br>",
    "text: ", as.character(favs$text), "<br>")}, "hover") %>%
  add_axis("x", orient = "top", ticks = 0, 
           title = "Hashtags by Favorite and Retweet Count",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0))) %>%
  add_axis("x", title = 'Favorite Count', properties = axis_props
           (labels =list(angle = 60, align = "left", baseline = "middle"))) %>%
   add_axis("y", title = 'Reply Count', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))

```

Then we created a table to display the tweets that have the most likes.

```{r}

most_favs <- 
tweets %>%
  select(screen_name, text, favorite_count, retweet_count, reply_count, quote_count) %>%
  filter(favorite_count >= 5) %>%
  arrange(desc(favorite_count))

formattable(most_favs)

```

We will also create a table to see the tweets that are shared the most.

```{r}
most_retweets <-
tweets %>%
  select(screen_name, text, favorite_count, retweet_count, reply_count, quote_count) %>%
  filter(retweet_count >= 5) %>%
  arrange(desc(retweet_count))

formattable(most_retweets)

```

# User Results

Now exploring the user data
```{r, include = FALSE, echo = FALSE}
#loc_users <-
#users %>%
  #group_by(location) %>%
  #tally() %>%
  #filter(n >= 2) %>%
  #tidyr::drop_na() %>%
  #arrange(desc(n))

#loc_users
```


Many of these accounts represent organizations and activists groups. I'm filtering by status count, which means the users who post the most tweets will be listed first.

```{r, include = FALSE}

#users %>%
  #select(screen_name, name, description, statuses_count, followers_count) %>%
  #group_by(screen_name) %>%
  #filter(statuses_count >= 5) %>%
  #arrange(desc(followers_count))

```

#  Voting Results

Twitter has a strong level of support for Initiative 1631, but the voting results are very different. Below is a chart that displays the overall sum of votes for Yes and No.
```{r}

sum_votes <- 
votes %>%
  group_by(Candidate) %>%
  mutate(Total_votes = sum(Votes)) 

ggplot(data = sum_votes, aes(x= Candidate, y = Total_votes))+
  geom_bar(stat = "identity",  position=position_dodge(), fill = "darkgreen") +
    geom_text(
    aes(label = Total_votes),
    position = position_dodge(1),
    vjust = 0) +
  ggtitle("Initiative 1631 Voting Results") +
  xlab("") +
  ylab("Total Votes") +
  theme_classic()

```

When we look at the results by county, we see that there is significant opposition to the measure in almost every county in the state.

```{r}
#option 1  
ggplot(votes, aes(x=County, y= Votes)) +
 geom_bar(stat="identity") +
 facet_wrap(~ Candidate, scale = "free", ncol = 4) + coord_flip()

#option 2
ggplot(votes, aes(County, Votes)) +
geom_bar(stat = "identity", aes(fill = Candidate)) + coord_flip()

```

## Mapping
We want to map the voting results to see how it differs from the tweets.

```{r}
target <- c("State Measures Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

target2 <- c("Washington State Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

votes <-
  votes %>%
  select(County, Race, Candidate, Votes, PercentageOfTotalVotes) %>%
  filter(Race %in% c(target, target2)) %>%
  mutate(subregion= tolower(County))


```

Then we need to get a map of Washington state.

```{r}

#i use the ggmap package for this map
state = c(left = -124.84, bottom = 45.54,right = -116.92, top = 49.0)

map2 <- get_stamenmap(state, zoom = 7, maptype = "toner-lite") %>% ggmap()

#this map information is pulled from ggmap's map package
county<- map_data("county")

wash_counties <-
county %>%
  filter(region == "washington") 

map2 +
  geom_path(data = wash_counties, mapping =aes(x= long, y= lat, group = group))

```

```{r}
w <- ggplot(data = wash_counties, mapping =aes(x= long, y= lat, group = group))


```

```{r}
#now we are merging the county data with the voting results into one file
vote_loc <- right_join(wash_counties, votes, by = "subregion")

```
Then I filter the data so that we can visualize the percentage of voters who voted Yes on 1631.

```{r}
yes_votes <-
  vote_loc %>%
  filter(Candidate == "Yes") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))

results <-  w +
  geom_polygon(data = yes_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +
  ggtitle("% Voters who Voted Yes on 1631")


results
```

We can also see where the many "No" Voters are


```{r}
no_votes <- 
  vote_loc %>%
  filter(Candidate == "No") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))

no_results <-  w +
  geom_polygon(data = no_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +     ggtitle("% Voters who Voted No on 1631")

no_results

```

trying to geocode the tweets

```{r}
users


```


## Discussion

Based on these findings, it appears that Initiative 1631 enjoyed far greater support among Twitter users than the voting population. However, the Twitter population is not a reprensentative sample of the voting population. Further, many of the supporters on Twitter represented activist groups who used it to advertise or drum up support for the carbon tax. Twitter also overrepresented urban and out-of-state residents, and voters in rural and even suburban areas are not well-represented on Twitter.


## References
