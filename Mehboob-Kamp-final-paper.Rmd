
---
title: "Fundamentals of Computing and Data Display"
author: "Rameesha Mehboob and Abigail Kamp"
date: "`r Sys.Date()`"
 output:
  pdf_document:
    df_print: kable
    toc: yes
  html_document:
    df_print: paged
    toc: yes
subtitle: SURV727 Mehboob and Kamp Final Presentation
---

## Introduction

In 2018, Initiative 1631 was on the ballot in Washington state. It asked voters:

"This measure would charge pollution fees on sources of greenhouse gas pollutants and use the revenue to reduce pollution, promote clean energy, and address climate impacts, under oversight of a public board.

Should this measure be enacted into law?""

The Initiative would have enacted a fee a $15 per ton of carbon emitted in 2020 and rise $2 per year until 2035. The revenue would have been invested in "clean air and energy", "clean water and health forests", and "healthy communities". Initiative 1631 was supported by a variety of climate groups, and received endorsements from many activist groups. There was a broad coalition supporting the measure in a state with left-leaning Washington politics, which gave supporters hope that it would pass. However, like many other tax increases, carbon taxes proved unpopular with voters. The referendum did not pass (56% to 44%) (Roberts 2018).

While this referendum was a defeat for climate activists, it is an interesting case study to compare the online support for a ballot initiative with actual voting results. For our analysis, we collect Twitter data about Initiative 1631 and compare it to the actual voting results in Washington state. 

```{r, include = FALSE}
#First we load in our packages
library(rtweet)
library(dplyr)
library(tidyr)
library(readr)
library(rjson)
library(ggmap)
library(maps)
library(ggvis)
library(ggplot2)
library(formattable)
```


## Data

First, we begin by collecting Twitter data. We each applied and obtained Twitter Enterprise accounts, which allowed us to access the entire Twitter archive. (For future researchers, each account is allowed free access to 5,000 Tweets. Once the limit is reached, Twitter will charge for further access.)

```{r, include = FALSE, eval = FALSE}
# Abby's Twitter API token. API information removed for security purposes
create_token(
  app = "SURV727",
  consumer_key = "",
  consumer_secret = "",
  access_token = "",
  access_secret = " ")

#get the token
get_token()
```

We pulled the Tweets by the four weeks before the election. We pulled more tweets closer to Election Day (November 06, 2018) because interest in an election generally increases as the date gets closer. 

``` {r, include = FALSE, eval = FALSE}
#Actually pulling the Tweets

##PULLING YES TWEETS##
yes_week1 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810090000",toDate = "201810152359")

yes_week2 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810160000",toDate = "201810222359")

yes_week3 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810230000",toDate = "201810292359")

yes_week4 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four yes objects and saving to one file
yes_4_weeks <- do.call("rbind", list(yes_week1, yes_week2, yes_week3, yes_week4))

##Pulling NEUTRAL Tweet"
neutral_week1 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 100, env_name = "SURV727", 
                                    fromDate ="201810090000",  toDate = "201810152359")

neutral_week2 <- search_fullarchive( "#initiative1631 OR (initiative 1631)", 
                                     n = 100, env_name = "SURV727",
                                     fromDate="201810160000", toDate = "201810222359")

neutral_week3 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810230000",toDate = "201810292359")

neutral_week4 <- search_fullarchive("#initiative1631 OR (initiative 1631)",
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810300000",toDate = "201811062359")

#merging the neutral weeks
netural_4_weeks <- do.call("rbind", list(neutral_week1, neutral_week2, neutral_week3, neutral_week4))

#Pulling the Negative Tweets#
no_week1 <- search_fullarchive("#NoOn1631 OR #Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810090000",toDate = "201810152359")

no_week2 <- search_fullarchive("NoOn1631 OR Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810160000",toDate = "201810222359")

no_week3 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810230000",toDate = "201810292359")

no_week4 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four no objects and saving to one file
no_4_weeks <- do.call("rbind", list(no_week1, no_week2, no_week3, no_week4))

#merging all tweets
all_tweets <- do.call("rbind", list(yes_4_weeks, netural_4_weeks, no_4_weeks))

#we then saved it as a CSV
save_as_csv(all_tweets,"tweets.csv",prepend_ids = TRUE, na = "",fileEncoding = "UTF-8")

```

Trying to do a final pull
We are doing one more pull to determine we get a natural sample of tweets, so we pull an extra 500 tweets that just mention Initiative 1631 to ensure that we have a more representative sample. This pull results in 490 tweets.

```{r, include = FALSE, eval = FALSE}
neutral_check <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 500, env_name = "SURV727", 
                                    fromDate ="201810090000",  toDate = "201811062359")

save_as_csv(neutral_check,"final-check-tweets.csv",prepend_ids = TRUE, na = "",fileEncoding = "UTF-8")

```

We take the new tweets and combine it with our other 1689 tweets. 

```{r, include = FALSE, eval = FALSE}
all_tweets <- rbind(tweets, neutral_check)

save_as_csv(all_tweets,"all_tweets.csv",prepend_ids = TRUE, na = "",fileEncoding = "UTF-8")

```

We check for duplicates by status_id, which is unique to each tweet. No duplicates were found. 

```{r, include = FALSE, eval = FALSE}
all_tweets <- all_tweets[!duplicated(all_tweets$status_id), ]

```

With the lat_lng function, we can also get the latitude and longitude of the tweet. This function from rtweet pulls all available latitude and longitude information for each tweet. However, this pull only results in two tweets with location information. This is because Twitter users need to allow Twitter to pull their geolocation and the vast majority of users opt out. 

```{r, include = FALSE, eval = FALSE}
new_location <- lat_lng(neutral_check)

```

We do not want to keep running the Twitter search_fullarchive function because it will eventually charge us money to pull more data, so we saved the data in CSV file. Then we removed duplicate tweets. There were about 400 duplicate tweets in the file.

```{r, include = FALSE}
# to avoid running the code again, we uploaded the Tweets file as CSV
#setwd()

tweets <- read_csv("all_tweets.csv")

#removing duplicates
tweets <- tweets[!duplicated(tweets$status_id), ]
```

We also obtained the user data that was associated with the Twitter accounts in the file above. The "users_data" function in rtweet pulls information about the individual users which we stored in a new file.

```{r, include = FALSE, results = "hide", message= FALSE}
#this won't work if you don't have the original R object, so commenting it out and adding our results as a CSV
#users <- users_data(tweets)
#set your working directory to where you have saved your data
#setwd("~/Data-cleaning/final-paper/data")

users <- read_csv("twitter-users.csv")
```

All location data in Twitter is input by the user, so it lacks uniformity and is difficult to work with. However, we start by trying to isolate the city from the user location data. Of note, this information is entered by the user and may be not accurate, but there is no way to verify location. 

To match the Twitter-user entered city, we need to match that with the relevant county information. We obtained a list of Washington cities and their respective counties from the Washington Court System Website. The Washington Court system lists each city and their county in alphabetically order, and we created a CSV from this information. 

```{r, include = FALSE}
#begin cleaning the user data by dropping the NAs and separating into city and state where possible
users <-
  users %>%
   dplyr::select(user_id, location, screen_name) %>%
   drop_na(.)

users <- separate(users, location, into = c("city","state"), sep= (","))

#need to get rid of the white space and creating a new file to do this
users2 <-
users %>%
  dplyr::select(user_id, screen_name, city) %>%
  mutate(city = tolower(trimws(.$city)))
```

We conduct more cleaning on the file. 

```{r, include= FALSE}
#uploading list of cities in washington to match with user information
cities <- read_csv("washington-cities.csv")

#renaming to match with other
colnames(cities)[1] <- "city"
colnames(cities)[2] <- "county"

#cleaning city data
cities <-
cities %>%
  mutate(city = tolower(trimws(.$city, "l")))

cities$city <- (gsub("\\s", "", cities$city))

str(users$city)
str(cities$city)

#comparing the locations dataframe
full_location <- merge(users2, cities, by = "city")

#reordering city variables
full_location <-
full_location %>%
  dplyr::select(user_id, screen_name, city, county)

#annoyingly full_location has an extra x in the user ID so need to remove
full_location$user_id <- gsub("x", "", full_location$user_id)
tweets$user_id <- gsub("x", "", tweets$user_id)

#merge user location with actual tweets
tweets_location <-
  full_location %>% semi_join(tweets, by = "user_id")

```

To obtain the voting results, we went to the Washington Secretary of State webpage and downloaded the "All Counties" file to obtain the results of each election broken down by county. (source: https://results.vote.wa.gov/results/20181106/Export.html) 

```{r, include = FALSE}
# download the file to your working directly and upload it as a CSV
votes <- read_csv("washington-2018-results.csv")
```


# Results

## Tweet Results

We created an  graph that shows the reply count and favorite count of the primary hashtags used by twitter users. Top 6 hashtags were extracted from 1686 tweets and their primary hashtags were the summed up for total favorite count and reply count. 

```{r, warning = FALSE, include = FALSE}

popular_tweets <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  group_by(hashtag1) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE)

popular_tweets_ggvis <-
popular_tweets%>%
  filter(hashtag1 >50)%>%
  ggvis(x= ~favorite_count,y= ~ reply_count) %>%
  filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "electionday",
                                    "climate",
                                    "electionday2018",
                                    "noon1631",
                                    "initiative1631"),
                                    label = "Choose Hashtag:",
                                    selected = c("yeson1631", 
                                    "electionday",
                                    "climate",
                                    "electionday2018",
                                    "noon1631",
                                    "initiative1631")))) %>% 

layer_points(fill = ~factor(hashtag1), size := input_numeric(80 , label = "Point size"))%>%
  add_tooltip(function(popular_tweets){paste0(
    ### <b>
    "Number of Favorite Count received: ", popular_tweets$favorite_count, "<br>",
     ### </b>
    "Number of Reply Count Received: ", popular_tweets$reply_count)}) %>%
  add_axis("x", title_offset = 40, title = "Number of Favorite Count Received", properties = axis_props(title = list(fontSize = 12))) %>%
  add_axis("y", title_offset = 40, title = "Number of Reply Count Received", properties = axis_props(title = list(fontSize = 12)))%>%
  add_axis("x", orient = "top", ticks = 0, title = "Twitter Support",
           properties = axis_props(
             axis = list(stroke = "white"),
             title = list(fontSize = 12),
             labels = list(fontSize = 0)))
  

```

```{r, warning= FALSE}
popular_tweets_ggvis
```

We also looked at the screen names of twitter users who are most actively participating in tweeting about Initiative 1631. The graph shows that there is  greater positive support for Initiative 1631. However most of the support is from several top users; which seems to be from activist groups rather than individuals. 

```{r, warning = FALSE, include = FALSE}
user_support <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count,text,screen_name) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  filter(hashtag1=="yeson1631"|hashtag1=="noon1631")%>%
  group_by(screen_name) %>%
  count(hashtag1)


user_support_ggvis <-
user_support%>%
  filter(n>2)%>%
ggvis(x = ~n, y = ~ screen_name,
       fill = ~continent,
        size := 100,
        opacity := 1) %>%
    filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "noon1631"),
                                    label = "Choose Hashtag:",
                                    selected = c("yeson1631",  "noon1631")))) %>% 
layer_points(fill = ~factor(hashtag1)) %>%
  add_axis('y', title=' ', properties=axis_props(labels=list(fontSize=12), title=list(fontSize=16,dy=-25)))%>%
  add_tooltip(function(user_support){paste0(
    ### <b>
    "Number of times hashtag was tweeted by user: ", user_support$n)})%>%
  add_axis("x", title_offset = 40, title = "Number of Tweets by Twitter User", properties = axis_props(title = list(fontSize = 12))) %>%
  add_axis("x", orient = "top", ticks = 0, title = "Evaluating Twitter Support with Screen Name ",
           properties = axis_props(
             axis = list(stroke = "white"),
             title = list(fontSize = 12),
             labels = list(fontSize = 0)))

```


```{r, warning= FALSE}
user_support_ggvis
```

We then plot the frequency of different hashtags. I start with the first hashtag that people use in the Tweet. Yeson1631 remains the most frequent hashtag.

```{r, warning = FALSE, include = FALSE}

hashes <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag1) %>%
  count(hashtag1) %>%
  arrange(desc(n))

first_tag <- 
hashes %>%
  filter(n >=5) %>%
  ggvis(x = ~n,
        y = ~hashtag1) %>%
layer_points(fill = ~factor(hashtag1)) %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag1, "<br>", "freq: ", hashes$n) }) %>%
  add_axis("x", title = 'Number of Uses', properties = axis_props(labels = list(angle = 0, 
             align = "left", baseline = "middle"))) %>%
  add_axis("x", orient = "top", ticks = 0, title = "Most Popular Hashtags",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0)))%>%
  add_axis("y", title = '', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))


```

```{r, warning = FALSE}
first_tag
```

Now looking at their second choice hashtag to see if the second hash tag mentioned in a post are different from the first. Based on our results, it looks like most of the hashtags remain positive.

```{r, warning = FALSE, include = FALSE}

hashes2 <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag2) %>%
  drop_na(hashtag2) %>%
  count(hashtag2) %>%
  arrange(desc(n))

hashes2

second_tag <- 
hashes2 %>%
  filter(n >=5) %>%
  ggvis(x = ~n,
        y = ~hashtag2) %>%
layer_points(fill = ~factor(hashtag2)) %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag2, "<br>", "freq: ", hashes$n) }) %>%
   add_axis("x", orient = "top", ticks = 0, title = "Second Most Popular Hashtags",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0))) %>%
  add_axis("x", title = 'Number of Uses', properties = axis_props
           (labels =list(angle = 0, align = "left", baseline = "middle"))) %>%
   add_axis("y", title = '', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))
```

```{r, warning= FALSE}
second_tag

```

Next we select  the most popular tweets and we graph the tweets as points to see which are the most Favorited and replied to. It will be useful to know if some are re tweeted more but aren't Favorited, or vice avers.

```{r, include = FALSE}

#Not to be included in the final paper
favs <-
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text, user_id) %>%
  tidyr::drop_na(favorite_count) %>%
  arrange(desc(favorite_count))

favs %>%
  filter(favorite_count >=10) %>%
  ggvis(x = ~favorite_count, y= ~reply_count, key := ~text) %>%
  layer_points() %>%
    add_tooltip(function(favs){paste0(
    "replies: ", favs$reply_count, "<br>",
    "favorites: ", favs$favorite_count, "<br>",
    "text: ", as.character(favs$text), "<br>")}, "hover") %>%
  add_axis("x", orient = "top", ticks = 0, 
           title = "Hashtags by Favorite and Retweet Count",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0))) %>%
  add_axis("x", title = 'Favorite Count', properties = axis_props
           (labels =list(angle = 60, align = "left", baseline = "middle"))) %>%
   add_axis("y", title = 'Reply Count', properties = axis_props(labels = list(angle = 0, 
             align = "right", baseline = "left")))

```

```{r, warning= FALSE}
favs
```

Then we created a table to display the tweets that have the most likes.

```{r, include = FALSE}

most_favs <- 
tweets %>%
  dplyr::select(screen_name, text, favorite_count, retweet_count, reply_count, quote_count) %>%
  filter(favorite_count >= 5) %>%
  arrange(desc(favorite_count))
```

```{r}
favs2 <- formattable(head(most_favs))

as.table(favs2)
```

We will also create a table to see the tweets that are shared the most.

```{r, include = FALSE}
most_retweets <-
tweets %>%
  dplyr::select(screen_name, text, favorite_count, retweet_count, reply_count, quote_count) %>%
  filter(retweet_count >= 5) %>%
  arrange(desc(retweet_count))
```

```{r, warning= FALSE}
most_retweets
```

```{r, include = FALSE, echo = FALSE}
loc_users <-
users %>%
  group_by(city) %>%
  tally() %>%
  filter(n >= 2) %>%
  tidyr::drop_na() %>%
  arrange(desc(n))

loc_users
```


#  Voting Results

```{r, include = FALSE}
target <- c("State Measures Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

target2 <- c("Washington State Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

votes <-
  votes %>%
  select(County, Race, Candidate, Votes, PercentageOfTotalVotes) %>%
  filter(Race %in% c(target, target2)) %>%
  mutate(subregion= tolower(County))

```

Twitter has a strong level of support for Initiative 1631, but the voting results are very different. Below is a chart that displays the overall sum of votes for Yes and No.
```{r, include = FALSE}

sum_votes <- 
votes %>%
  group_by(Candidate, Race) %>%
  mutate(Total_votes = sum(Votes)) 

vote_count <-
ggplot(data = sum_votes, aes(x= Candidate, y = Total_votes))+
  geom_bar(stat = "identity",  position=position_dodge(), fill = "darkgreen") +
    geom_text(
    aes(label = Total_votes),
    position = position_dodge(1),
    vjust = 0) +
  ggtitle("Initiative 1631 Voting Results") +
  xlab("") +
  ylab("Total Votes") +
  theme_classic()
```

```{r, warning= FALSE}
vote_count

```

When we look at the results by county, we see that there is significant opposition to the measure in almost every county in the state.

```{r, include = TRUE}
#option 
ggplot(votes, aes(County, Votes)) +
geom_bar(stat = "identity", aes(fill = Candidate)) + coord_flip()

```

## Mapping

We pull a map of Washington state and the coordinates for the country boundaries from the maps package. 

```{r, include = FALSE, warning= FALSE}

#i use the ggmap package for this map
state = c(left = -124.84, bottom = 45.54,right = -116.92, top = 49.0)

map2 <- get_stamenmap(state, zoom = 7, maptype = "toner-lite") %>% ggmap()

#this map information is pulled from ggmap's map package
county<- map_data("county")

wash_counties <-
county %>%
  filter(region == "washington") 

map2 +
  geom_path(data = wash_counties, mapping =aes(x= long, y= lat, group = group))

w <- ggplot(data = wash_counties, mapping =aes(x= long, y= lat, group = group))


```

Next we need to clean the voting data and combine it with the county data so we can map it.

```{r, inlude = TRUE, warning = FALSE}
votes <-
  votes %>%
  select(County, Race, Candidate, Votes, PercentageOfTotalVotes) %>%
  filter(Race %in% c(target, target2)) %>%
  mutate(subregion= tolower(County))

vote_loc <- right_join(wash_counties, votes, by = "subregion")


```

Then I filter the data so that we can visualize the percentage of voters who voted Yes on 1631 and map the results by county. This displays where people voted for Initiative 1631.

```{r, include=FALSE, echo = FALSE, warning=FALSE}
yes_votes <-
  vote_loc %>%
  filter(Candidate == "Yes") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))


results <-  w +
  geom_polygon(data = yes_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +
  ggtitle("% Voters who Voted Yes on 1631")

```

```{r}
results
```

We can also see where the many "No" Voters are located; there are many  No voters.


```{r, include=FALSE, echo = FALSE, warning=FALSE}
no_votes <- 
  vote_loc %>%
  filter(Candidate == "No") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))

no_results <-  w +
  geom_polygon(data = no_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +     ggtitle("% Voters who Voted No on 1631")

```

```{r, warning = FALSE}
no_results

```


## Discussion

Based on these findings, it appears that Initiative 1631 enjoyed far greater support among Twitter users than the voting population. However, the Twitter population is not a representative sample of the voting population. Further, many of the supporters on Twitter represented activist groups who used it to advertise or drum up support for the carbon tax. Twitter also over-represented urban and out-of-state residents, and voters in rural and even suburban areas are not well-represented on Twitter.


## References
Roberts, David. "Washington votes no on a carbon tax - again. Vox. November 6, 2018. "https://www.vox.com/energy-and-environment/2018/9/28/17899804/washington-1631-results-carbon-fee-green-new-deal

Washington Courts. "Court Directory: Country-City Reference List." https://www.courts.wa.gov/court_dir/?fa=court_dir.countycityref

