---
title: "Fundamentals of Computing and Data Display"
subtitle: "SURV727 Mehboob and Kamp Final Presentation"
author: "Rameesha Mehboob and Abigail Kamp"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
references:
- id: Wickham2014
  title: Tidy Data
  author:
  - family: Wickham
    given: Hadley
  container-title: Journal of Statistical Software
  volume: 59
  issue: 10
  page: 1-23
  type: article-journal
  issued:
    year: 2014
- id: Baumer2017
  title: Modern Data Science with R
  author:
  - family: Baumer
    given: Benjamin S.
  - family: Kaplan
    given: Daniel T.
  - family: Horton
    given: Nicholas J.
  type: book
  publisher: Chapman \& Hall/CRC Press.
  issued:
    year: 2017
---

## Introduction

In 2018, Initiative 1631 was on the ballot in Washington state. It asked voters:

"This measure would charge pollution fees on sources of greenhouse gas pollutants and use the revenue to reduce pollution, promote clean energy, and address climate impacts, under oversight of a public board.

Should this measure be enacted into law?""

The Initiative would have enacted a fee a $15 per ton of carbon emitted in 2020 and rise $2 per year until 2035. The revenue would have been invested in "clean air and energy", "clean water and health forests", and "healthy communities". Initiative 1631 was supported by a variety of climate groups, and recieved endorsements from many activist groups. There was a broad coalition supporting the measure in a state with left-leaning Washington politics, which gave supporters hope that it would pass. However, like many other tax increases, carbon taxes proved unpopular with voters. The referendum did not pass (56% to 44%). (Source:  https://www.vox.com/energy-and-environment/2018/9/28/17899804/washington-1631-results-carbon-fee-green-new-deal)

While this referendum was a defeat for climate activists, it is an interesting case study to compare the online support for a ballot initiative with actual voting results. For our analysis, we collect Twitter data about Initative 1631 and compare it to the actual voting results in Washington state. 

```{r, include = FALSE}
#First we load in our Twitter packages
library(rtweet)
library(dplyr)
library(tidyr)
library(readr)
library(rjson)
library(ggmap)
library(maps)
library(ggvis)
```


## Data

First, we begin by collecting Twitter data. We each applied and obtained Twitter Enterprise accounts, which allowed us to access the entire Twitter archive. (For future researchers, each account is allowed free access to 5,000 Tweets. Once the limit is reached, TWitter will charge for further aceess.)

```{r, include = FALSE}
# Abby's Twitter API token. API information removed for security purposes
create_token(
  app = "SURV727",
  consumer_key = "",
  consumer_secret = "",
  access_token = "",
  access_secret = " ")

#get the token
get_token()
```

We pulled the Tweets by the four weeks before the election. We pulled more tweets closer to Election Day (November 06, 2018) because interest in an election generally increases as the date gets closer.

``` {r, include = FALSE}
#Actually pulling the Tweets

##PULLING YES TWEETS##
yes_week1 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810090000",toDate = "201810152359")

yes_week2 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 100, env_name = "SURV727",
                                fromDate ="201810160000",toDate = "201810222359")

yes_week3 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810230000",toDate = "201810292359")

yes_week4 <- search_fullarchive("YesOn1631 OR Yesto1631", n = 250, env_name = "SURV727",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four yes objects and saving to one file
yes_4_weeks <- do.call("rbind", list(yes_week1, yes_week2, yes_week3, yes_week4))

##Pulling NEUTRAL Tweet"
neutral_week1 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 100, env_name = "SURV727", 
                                    fromDate ="201810090000",  toDate = "201810152359")

neutral_week2 <- search_fullarchive( "#initiative1631 OR (initiative 1631)", 
                                     n = 100, env_name = "SURV727",
                                     fromDate="201810160000", toDate = "201810222359")

neutral_week3 <- search_fullarchive("#initiative1631 OR (initiative 1631)", 
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810230000",toDate = "201810292359")

neutral_week4 <- search_fullarchive("#initiative1631 OR (initiative 1631)",
                                    n = 250, env_name = "SURV727", 
                                    fromDate ="201810300000",toDate = "201811062359")

#merging the neutral weeks
netural_4_weeks <- do.call("rbind", list(neutral_week1, neutral_week2, neutral_week3, neutral_week4))

#Pulling the Negative Tweets#
no_week1 <- search_fullarchive("#NoOn1631 OR #Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810090000",toDate = "201810152359")

no_week2 <- search_fullarchive("NoOn1631 OR Noto1631", n = 100, env_name = "Dev2",
                                fromDate ="201810160000",toDate = "201810222359")

no_week3 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810230000",toDate = "201810292359")

no_week4 <- search_fullarchive("NoOn1631 OR Noto1631", n = 250, env_name = "Dev2",
                                fromDate ="201810300000",toDate = "201811062359")

#merging the four no objects and saving to one file
no_4_weeks <- do.call("rbind", list(no_week1, no_week2, no_week3, no_week4))

#merging all tweets
all_tweets <- do.call("rbind", list(yes_4_weeks, netural_4_weeks, no_4_weeks))

#we then saved it as a CSV
save_as_csv(all_tweets,"tweets.csv",prepend_ids = TRUE, na = "",fileEncoding = "UTF-8")

```

We also obtained the user data that was associated with the Twitter accounts in the file above. The "users_data" function in rtweet pulls information about the individual users which we stored in a new file.

```{r, include = FALSE}
#this won't work if you don't have the original R object, so commenting it out and adding our results as a CSV
#users <- users_data(tweets)

users <- read_csv("twitter-users.csv")
```

To obtain the voting results, we went to the Washington Secretary of State webpage and downloaded "All Counties" to obtain the results of each election broken down by county. (source: https://results.vote.wa.gov/results/20181106/Export.html) 

We also created a list of cities in Washington into an Excel from the following list. 

```{r, include = FALSE}
# download the file to your working directly and upload it as a CSV
votes <- read_csv("~/Data-cleaning/final-paper/data/washington-2018-results.csv")
```

```{r, include = FALSE}
# to avoid running the code again, we uploaded the Tweets file as CSV
#setwd()
tweets <- read_csv("Data-cleaning/final-paper/data/tweets.csv")

#removing duplicates
tweets <- tweets[!duplicated(tweets$status_id), ]
```


## Results

This section presents the main results.

We created an interactive graph that shows the reply count and favorite count of the primary hashtags used by twitter users. Top 6 hashtags were extracted from 1881 tweets and their primary hashtags were the summed up for total favorite count and reply count. 

```{r}

popular_tweets <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count, text) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  group_by(hashtag1) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE)

popular_tweets%>%
  filter(hashtag1 >50)%>%
  ggvis(x= ~favorite_count,y= ~ reply_count) %>%
  filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "electionday",
                                    "climate",
                                    "electionday2018",
                                    "noon1631",
                                    "initiative1631"),
                                    label = "Choose Hashtag:",
                                    selected = "yeson1631"))) %>% 

layer_points(fill = ~factor(hashtag1), size := input_numeric(60 , label = "Point size"))%>%
  add_tooltip(function(popular_tweets){paste0(
    ### <b>
    "Number of favorite count received: ", popular_tweets$favorite_count, "<br>",
     ### </b>
    "Number of reply count received: ", popular_tweets$reply_count)})
  

```

We also looked at the screen_names of twitter users who are most actively participating in tweeting about initiative 1631. The graph shows that there is much greater positive support for initiative 1631. However most of the support is from a few top users; which seems to be from climate groups rather than individuals. 

```{r}
user_support <- 
  tweets %>%
  dplyr::select(hashtags, reply_count, favorite_count,text,screen_name) %>%
  drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ")%>%
  filter(hashtag1=="yeson1631"|hashtag1=="noon1631")%>%
  group_by(screen_name) %>%
  count(hashtag1)



user_support%>%
  filter(n>2)%>%
ggvis(x = ~n, y = ~ screen_name,
       opacity := input_slider(0, 1, label = "Opacity",value=1),
       size := input_slider(1, 100, value = 50, step = 10, 
                            label = "Point Size")) %>%
    filter(hashtag1%in% 
           eval(input_checkboxgroup(c("yeson1631",
                                    "noon1631"),
                                    label = "Choose Hashtag:",
                                    selected = "yeson1631"))) %>% 
layer_points(fill = ~factor(hashtag1)) %>%
  add_axis('y', title=' ', properties=axis_props(labels=list(fontSize=12), title=list(fontSize=16,dy=-25)))%>%
  add_tooltip(function(user_support){paste0(
    ### <b>
    "Number of times hashtag was tweeted by user: ", user_support$n)})

```

Plotting the frequency of different hashtags. I start with the first hashtag that people use. Yeson1631 remains the most frequent hashtag.

```{r}
#manipulating my dataset to get the hashtags
hashes <-
  tweets %>%
  select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag1) %>%
  count(hashtag1) %>%
  arrange(desc(n))

first_tag <- 
hashes %>%
  filter(n >=5) %>%
  ggvis(x = ~hashtag1,
        y = ~n,
        fill= ~n) %>%
  layer_points() %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag1, "<br>", "freq: ", hashes$n) }) %>%
  add_axis("x", title = '', properties = axis_props(labels = list(angle = 60, 
             align = "left", baseline = "middle"))) %>%
  add_axis("x", orient = "top", ticks = 0, title = "Most Popular Hashtags",
           properties = axis_props(
             axis = list(stroke = "white"),
             labels = list(fontSize = 0)))

first_tag
```

Now looking at their second choice hastag to see if the second hashtag mentioned in a post are different from the first. Based on our results, it looks like most of the hashtags remain positive.

```{r}

hashes2 <-
  tweets %>%
  select(hashtags, reply_count, favorite_count, text) %>%
  tidyr::drop_na(hashtags) %>%
  mutate(hashtags = tolower(hashtags)) %>%
  tidyr::separate(., hashtags, into = c("hashtag1", "hashtag2", "hashtag3"),
           sep = " ") %>%
  group_by(hashtag2) %>%
  drop_na(hashtag2) %>%
  count(hashtag2) %>%
  arrange(desc(n))

hashes

second_tag <- 
hashes2 %>%
  filter(n >=5) %>%
  ggvis(x = ~hashtag2,
        y = ~n,
        fill= ~n) %>%
  layer_points() %>%
  add_tooltip(function(hashes){paste0(
    "hashtag: ", hashes$hashtag2, "<br>", "freq: ", hashes$n) }) %>%
  add_axis("x", properties = axis_props(labels = list(angle = 60, 
             align = "left", baseline = "middle")))

second_tag

```

Now selecting for the most  popular tweets and graphing the tweets as points to see which are the most favorited and replied to. 

```{r}
favs <-
  tweets %>%
  select(hashtags, reply_count, favorite_count, text, user_id) %>%
  tidyr::drop_na(favorite_count) %>%
  arrange(desc(favorite_count))

favs %>%
  filter(favorite_count >=10) %>%
  ggvis(x = ~favorite_count, y= ~reply_count, key := ~text) %>%
  layer_points() %>%
    add_tooltip(function(favs){paste0(
    "replies: ", favs$reply_count, "<br>",
    "favorites: ", favs$favorite_count, "<br>",
    "text: ", as.character(favs$text), "<br>")}, "hover")
```
## Mapping
We want to map the voting results to see how it differs from the tweets.

```{r}
target <- c("State Measures Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

target2 <- c("Washington State Initiative Measure No. 1631 Initiative Measure No. 1631 concerns pollution.")

votes <-
  votes %>%
  select(County, Race, Candidate, Votes, PercentageOfTotalVotes) %>%
  filter(Race %in% c(target, target2)) %>%
  mutate(subregion= tolower(County))


```

Then we need to get a map of Washington state.

```{r, include = FALSE}

#i use the ggmap package for this map
state = c(left = -124.84, bottom = 45.54,right = -116.92, top = 49.0)

map2 <- get_stamenmap(state, zoom = 7, maptype = "toner-lite") %>% ggmap()
map2

#this map information is pulled from ggmap's map package
county<- map_data("county")

wash_counties <-
county %>%
  filter(region == "washington") 

#can we map these counties on top of our washington state map?
map2+
  geom_path(aes(x= long, y= lat, group = group), data = wash_counties)

w <- ggplot(data = wash_counties, mapping =aes(x= long, y= lat, group = group))
#now we are merging the county data with the voting results into one file
vote_loc <- right_join(wash_counties, votes, by = "subregion")

head(vote_loc)

```

Then I filter the data so that we can visualize the percentage of voters who voted Yes on 1631.

```{r}
yes_votes <-
  vote_loc %>%
  filter(Candidate == "Yes") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))

results <-  w +
  geom_polygon(data = yes_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +
  ggtitle("% Voters who Voted Yes on 1631")


results
```
We can also see where the many "No" Voters are


```{r}
no_votes <- 
  vote_loc %>%
  filter(Candidate == "No") %>%
  mutate(PercentageOfTotalVotes = as.numeric(.$PercentageOfTotalVotes)) %>%
  mutate(Votes = as.numeric(.$Votes))

no_results <-  w +
  geom_polygon(data = no_votes, aes(x= long, y= lat, fill = PercentageOfTotalVotes)) +     ggtitle("% Voters who Voted No on 1631")

no_results


```

```{r}
# What happens here depends on the specific project
```
## Discussion

This section summarizes the results and may briefly outline advantages and limitations of the work presented.

## References
